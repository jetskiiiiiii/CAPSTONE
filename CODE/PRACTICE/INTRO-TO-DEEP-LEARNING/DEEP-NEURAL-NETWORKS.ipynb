{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "Neural networks organize their neurons into *layers*. \n",
    "\n",
    "Linear units having a common set of inputs comprise a *dense* layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Activation Function\n",
    "\n",
    "Dense layers by themselves are only capable of linear models.\n",
    "\n",
    "Activation functions are applied to each layers' output to help the model fit to curves.\n",
    "\n",
    "<br/>\n",
    "\n",
    "The most common activation function is the *rectifier function*.\n",
    "\n",
    "When applied to a linear unit (a layer), we get a *rectified linear unit* or *ReLU*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Dense Layers\n",
    "\n",
    "Layers are made before the output layer, which are referred to as *hidden layers*.\n",
    "\n",
    "<br/>\n",
    "\n",
    "If the final layer is a linear unit, then the network is fit for regression. \n",
    "\n",
    "For other tasks such as regression, an activation function might be required on the output/final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Sequential model connects a list of layers in order from first to last\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # 1st hidden layer with 4 neurons\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    # 2nd hidden layer with 3 neurons \n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the activation function can be on its own line (in case some other layer needs to be in-between hidden layer and activation layer)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # 1st hidden layer with 4 neurons\n",
    "    layers.Dense(units=4, input_shape=[2]),\n",
    "    # 1st activation layer\n",
    "    layers.Activation('relu'),\n",
    "    # 2nd hidden layer with 3 neurons \n",
    "    layers.Dense(units=3),\n",
    "    # 2nd activation layer\n",
    "    layers.Activation('relu'),\n",
    "    # the linear output layer\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
